% !TEX root = ../article.tex

\subsubsection{Session 3: {\tt feval} optimization in McVM}

McVM is a virtual machine for MATLAB developed at McGill University. As a by-product of our project, we ported it from the LLVM legacy JIT to MCJIT, and later extended it with a new specialization mechanism for {\tt feval} calls. The source code for this version along with the MATLAB benchmarks listed in \mysection\ref{ss:bench-setup} are publicly available at \url{https://github.com/dcdelia/mcvm}.

Experiments reported in \mytable\ref{tab:feval} (Question Q4) can be repeated using a number of scripts provided along with a McVM build in {\small\tt /home/osrkit/Desktop/mcvm/}. %Pre-requirements for McVM compilation are header files for a number of scientific libraries (ATLAS, BLAS, and LAPACKE) and the Boehm garbage collector, which can be built automatically using the script {\tt bootstrap.sh} provided in the repository.

\noindent For each benchmark {\tt X}, {\small\tt benchmarks/scripts/} contains three MATLAB scripts to use as input for {\tt mcvm}:

\begin{itemize}[parsep=0pt]
\item {\tt base/X}: single run of original code (i.e., {\tt feval}-based);
\item {\tt direct/X}: single run of code optimized by hand (i.e., with direct calls);
\item {\tt many/X}: multiple runs of original code (for code caching).
\end{itemize}

\noindent We manually collected figures from the console output and computed speedups for the different settings. We show how to run the code using {\tt odeRK4} as an example. The platform used to obtain reported numbers is the same as in session 2.

To determine a baseline for speedup computation, we let {\tt mcvm} perform a single run of the original code with no {\tt feval} optimization. Note that we can selectively enable or disable {\tt feval} optimization using the {\tt -jit\_feval\_opt} flag:

%***********************************************
%     McVM - The McLab Virtual Machine v1.0
%Visit http://www.sable.mcgill.ca for more info.
%***********************************************

\begin{small}
\begin{verbatim}
$ cd ~/Desktop/mcvm
$ ./mcvm -jit_feval_opt false <
         benchmarks/scripts/base/odeRK4
[...]
>: >: Compiling function: "testSH"
Compiling function: "odeRK4"
Compiling function: "testSHfun"
Compiling function: "rhsSteelHeat"
Compiling function: "testSHfun"
Compiling function: "rhsSteelHeat"
[TOC] Elapsed time: 20.141959 seconds
 t y_RK4
   0.0000  1.000000
  20.0000 227.364633
\end{verbatim}
\end{small}

\noindent To measure the performance of McVM's code caching mechanism, we let the benchmark run multiple times in the same instance of the VM:

\begin{small}
\begin{verbatim}
$ ./mcvm -jit_feval_opt false <
         benchmarks/scripts/many/odeRK4
\end{verbatim}
\end{small}

\noindent The experiment duration on our platform was $\approx2$m, with an average time per trial of $\approx 19.836$s (manually computed by averaging the elapsed time figures from the console, after discarding the warm-up run). The resulting speedup for the base code caching mechanism was thus $20.142/19.836=1.015\times$, slightly different than the one reported in column {\em Base} of \mytable\ref{tab:feval} for the Intel Xeon platform, for which we repeated each experiment $10$ times.

We can now set an upper bound for speedups by measuring the running time when the code has been optimized by hand inserting direct calls in place of {\tt feval} instructions:

\begin{small}
\begin{verbatim}
$ ./mcvm < benchmarks/scripts/direct/odeRK4
[...]
>: >: Compiling function: "testSH_direct"
Compiling function: "odeRK4_testSHfun"
Compiling function: "testSHfun"
Compiling function: "rhsSteelHeat"
[TOC] Elapsed time: 7.977169 seconds
 t y_RK4
   0.0000  1.000000
  20.0000 227.364633
\end{verbatim}
\end{small}

\noindent In this scenario McVM can compile the whole program ahead of time, as {\tt rhsSteelHeat} is not invoked through an {\tt feval} instruction anymore. A comparison of the running times suggests a rough $20.142/7.977=2.525\times$ speedup for by-hand optimization w.r.t. the baseline version (compare to column {\em Direct} in \mytable\ref{tab:feval}).

We can now try to assess the speedup from our {\tt feval} optimization technique on {\tt odeRK4}:

\begin{small}
\begin{verbatim}
$ cd ~/Desktop/mcvm
$ ./mcvm -jit_feval_opt true <
         benchmarks/scripts/base/odeRK4
[...]
>: >: Compiling function: "testSH"
Compiling function: "odeRK4"
Compiling and tracking a feval instruction...
Compiling and tracking a feval instruction...
Compiling and tracking a feval instruction...
Compiling and tracking a feval instruction...
Function contains annotated feval instructions!
Compiling function: "testSHfun"
Compiling function: "rhsSteelHeat"
Type conversion required for variable y
Type conversion required for variable $t10
[TOC] Elapsed time: 8.450570 seconds
 t y_RK4
   0.0000  1.000000
  20.0000 227.364633
\end{verbatim}
\end{small}

\noindent The execution time ratio between the base version and the optimized code that we JIT-compile is thus $20.142/8.451=2.383$ (compare to column {\em Opt. JIT} in \mytable\ref{tab:feval}). Notice that compensation code is generated to perform unboxing of IIR variables {\tt y} and {\tt \$t10} (``Type conversion required...'') so that execution can correctly resume from the optimized code.

We can finally evaluate the speedup enabled by our code caching mechanism (\mysection\ref{ss:eval-opt-mcvm}) for the compilation of continuation functions by running:
\begin{small}
\begin{verbatim}
$ ./mcvm -jit_feval_opt true <
         benchmarks/scripts/many/odeRK4
\end{verbatim}
\end{small}

\noindent The experiment duration was $\approx1$m, with a time per trial of $\approx11.817$s (discarding the warm-up run). The resulting speedup w.r.t. is thus $20.142/8.006=2.516\times$ (compare to column {\em Opt. cached} in \mytable\ref{tab:feval}).

