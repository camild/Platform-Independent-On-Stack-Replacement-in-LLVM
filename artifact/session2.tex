% !TEX root = ../article.tex

\subsubsection{Session 2: Performance Figures}

The experiments can be repeated by executing scripts on a selection of the \shootout\ benchmarks~\cite{shootout}. Each benchmark was compiled in {\tt clang} with both {\tt -O0} and {\tt -O1}. For each benchmark {\tt X}, {\tt tinyvm/shootout/X/} contains the unoptimized and optimized ({\tt -O1}) IR code, each in two versions:

\begin{itemize}[parsep=0pt]
\item {\tt bench} and {\tt bench-O1}: IR code of the benchmark;
%\item {\tt codeQuality}: IR code of the benchmark with the hottest loop instrumented with a never-firing OSR;
\item {\tt finalAlwaysFire} and {\tt finalAlwaysFire-O1}: IR code of the benchmark preprocessed by turning the body of the hottest loop into a separate function (see \ref{ss:experim-results}).
\end{itemize}

\noindent Each experiment runs a warm-up phase followed by 10 identical trials. We manually collected the figures from the console output and analyzed them, computing confidence intervals. We show how to run the code using {\tt n-body} as an example. Times reported in this section have been measured in VirtualBox on an Intel Core i7 platform, a different setup than the one discussed in \ref{ss:bench-setup}.

\paragraph{Question Q1.} The purpose of the experiment is assessing the impact on code quality due to the presence of OSR points.
The first step consists in generating figures for the baseline (uninstrumented) benchmark version:
\begin{small}
\begin{verbatim}
tinyvm$ tinyvm shootout/scripts/bench/n-body
\end{verbatim}
\end{small}

\noindent The script is as follows:

\begin{small}
\begin{verbatim}
LOAD_IR shootout/n-body/bench.ll
bench(50000000)
REPEAT 10 bench(50000000)
QUIT
\end{verbatim}
\end{small}

\noindent which loads the IR code, performs a warm-up execution of the benchmark, and then 10 repetitions. The experiment duration is $\approx1$m, with a time per trial $\approx5.725$s. 

The benchmark with the hottest loop instrumented with a never-firing OSR can be run as follows:

\begin{small}
\begin{verbatim}
tinyvm$ tinyvm shootout/scripts/codeQuality/n-body
\end{verbatim}
\end{small}

\noindent The script is as follows:

\begin{small}
\begin{verbatim}
LOAD_IR shootout/n-body/bench.ll
INSERT_OSR 5 NEVER OPEN UPDATE IN bench AT %8 CLONE
bench(50000000)
REPEAT 10 bench(50000000)
QUIT
\end{verbatim}
\end{small}

\noindent The experiment duration is $\approx1$m with a time per trial: $\approx5.673$s. The ratio $5.673/5.725=0.990$ for {\tt n-body} is slightly smaller than the one reported in \ref{fig:code-quality-base} on the Intel Xeon. The experiment for building \ref{fig:code-quality-O1} uses scripts in {\tt bench-O1} and {\tt codeQuality-O1}.

\paragraph{Question Q2.} This experiment assesses the run-time overhead of an OSR transition by measuring the duration of an always-firing OSR execution and of a never-firing OSR execution, and reporting the difference averaged over the number of fired OSRs. The script for this is:
\begin{small}
\begin{verbatim}
tinyvm$ tinyvm shootout/scripts/bench/n-body
\end{verbatim}
\end{small}

\paragraph{Question Q3.} 


%[Q2] What is the run-time overhead of an OSR transition, for instance to a clone of the running function?
%[Q3] What is the overhead of \osrkit\ for inserting OSR points and creating a stub or a continuation function?
%[Q4] What kind of benefits can we expect by using OSR in a production environment based on LLVM?

