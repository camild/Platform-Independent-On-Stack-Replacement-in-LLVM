% !TEX root = article.tex

\section{Experimental Evaluation}
\label{se:experiments}

In this section we present a preliminar experimental study of our OSR technique in TinyVM, a proof-of-concept virtual machine based on LLVM's JIT compiler MCJIT. TinyVM supports interactive invocations of functions and it can compile LLVM IR either generated at run-time or loaded from disk. The main design goal behind TinyVM is the creation of an interactive environment for IR manipulation and JIT-compilation of functions: for instance, it allows the user to insert OSR points in loaded functions, run optimization passes on them or display their CFGs, repeatedly invoke a function for a specified amount of times and so on. TinyVM supports dynamic library loading and linking, and comes with a helper component for MCJIT that simplifies tasks such as handling multiple IR modules, symbol resolution in presence of multiple versions of a function, and tracking native code and other machine-level generated object such as Stackmaps.

TinyVM is thus an ideal playground to exercise our OSR technique, and we use it to run performance measurements on the shootout test suite, also known as the Computer Language Benchmark Game~\cite{shootout}. The list of benchmarks and their description is reported in Table~\ref{tab:shootout}; four of them - namely {\tt b-trees}, {\tt mbrot}, {\tt n-body} and {\tt sp-norm} - are evaluated against two workloads of different size.

\begin{table} 
\begin{small}
    \begin{tabular}{ |c|c| }
        \hline
        {\em Benchmark} & {\em Description} \\ 
        \hline
        \hline
        b-trees & Adaptation of a GC bench for binary trees \\ 
        \hline
        fannkuch & Fannkuch benchmark on permutations \\ 
        \hline
        fasta & Generation of DNA sequences \\ 
        \hline
        fasta-redux & Generation of DNA sequences (with lookup table) \\ 
        \hline
        mbrot & Mandelbrot set generation \\ 
        \hline
        n-body & N-body simulation of Jovian planets \\ 
        \hline
        rev-comp & Reverse-complement of DNA sequences \\ 
        \hline
        sp-norm & Eigenvalue calculation with power method \\ 
        \hline
    \end{tabular} 
\end{small}
\caption{\label{tab:shootout} Description of the shootout benchmarks} 
\end{table}

\subsection{Setup}

We generated the IR modules for our experiments with clang, starting from the C version of the shootout suite. No LLVM optimization passes were performed on the code other than {\em mem2reg}, which promotes memory references to be register references and is typically used in SSA form construction.

Experiments were performed on an octa-core 2.3 Ghz Intel Xeon E5-4610 v2 with 256+256KB of L1 cache, 2MB of L2 cache, 16MB of shared L3 cache and 128 GB of DDR3 main memory, running Debian Wheezy 7, Linux kernel 3.2.0, LLVM 3.6.2 (Release build, compiled using gcc 4.7.2), 64 bit.

For each benchmark we performed 10 trials preceded by an initial warm-up iteration; reported confidence intervals are stated at 95\% confidence level.

\subsection{Results}

\begin{itemize}
\item {\bf Message 1}: how much does a never-firing OSR point impact code quality? We run a program with one or more OSR points, and we measure the slowdown given by factors such as cache effects (due to code bloat), register pressure, etc. due to the presence of the OSR points.
\item {\bf Message 2}: what is the overhead of an OSR transition to the same function? We run a program with a controlled OSR transition, e.g., with a counter that fires the OSR. Here we measure the impact of the actual OSR call [we already tried this with the repeated addition microbenchmark simple\_loop\_SSA.ll]. We compute for each benchmark: 1) the average time per OSR transition; 2) the number of transferred live variables; 3) the total benchmark time with an always-firing OSR at each iteration of the hottest loop; 4) the total benchmark time without OSR instrumentation (baseline); 5) the number of iterations of the hottest loop (equals the number of OSR transitions).
\item {\bf Message 3}: what is the overhead of the library for inserting OSR points? We compute for each benchmark: 1) time required by insertOpenOSR (OSR point insertion + creates stub); 2) time required by insertFinalizedOSR (OSR point insertion + generation of contination function); 3) time required for generating continuation function.
\end{itemize}

\paragraph{Impact on code quality}
In order to measure how much a never-firing OSR point might impact code quality, we analyzed the source-code structure of each benchmark and profiled its run-time behavior to identify performance-critical sections for OSR point insertion.

For iterative benchmarks, we insert an OSR point in the body of their hottest loops. We classify a loop as hottest when its body is executed for a very high cumulative number of iterations (e.g., from a few thousands up to billions) and it either calls the method with the highest {\em self} time in the program, or it performs the most computational-intensive operations for the program in its own body. These loops are natural candidates for OSR point insertion, as they can be used - as in the Jikes RVM - to enable more dynamic inlining opportunities, with the benefits from several control-flow (e.g., dead code elimination) and data-flow (e.g., constant propagation) optimizations based on the run-time values of the live variables. In the shootout benchmarks, the number of such loops is typically 1 (2 for {\tt spectral-norm}).

For {\tt b-trees} - the only benchmark in our suite showing a recursive pattern - we insert an OSR point in the body of the method that accounts for the largest {\em self} execution time of the program. Such an OSR point might be useful to trigger recompilation of the code at a higher degree of optimization, or to enable some form of dynamic optimization (for instance, in a recursive search algorithm we might want to inline the comparator method provided by the user at the call).
  
\paragraph{Overhead of OSR transitions}

In this [...]

\begin{table} 
\begin{footnotesize}
    \begin{tabular}{ |c|c|c|c| }
        \cline{3-4}
        \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{Base version} \\
        \hline
        Benchmark & Fired OSR transitions & Live values & Avg time (ns) \\ 
        \hline
        \hline
        b-trees & 605\,377\,887 & 2 & 3.057 \\ 
        \hline
        b-trees-large & 2\,689\,946\,975 & 2 & 2.986 \\ 
        \hline
        fannkuch & ??? & ??? & ??? \\ 
        \hline
        fasta & 400\,000\,000 & 2 & 3.039 \\ 
        \hline
        fasta-redux & 400\,000\,000 & 4 & 2.566 \\ 
        \hline
        mbrot & 256\,000\,000 & 15 & 6.553 \\ 
        \hline
        mbrot-large & 1\,024\,000\,000 & 15 & 6.345 \\ 
        \hline
        n-body & 50\,000\,000 & 3 & 2.543 \\ 
        \hline
        n-body-large & 500\,000\,000 & 3 & 2.329 \\ 
        \hline
        rev-comp & 6\,172\,843 & 8 & 3.589 \\ 
        \hline
        sp-norm & 1\,210\,000\,000 & 2 & 0.752 \\ 
        \hline
        sp-norm-large & 19\,360\,000\,000 & 2 & 0.756 \\
        \hline
    \end{tabular} 
\end{footnotesize}
\caption{\label{tab:sameFun}Average cost of an OSR transition to the same function.} 
\end{table}

% figure
\ifdefined\noauthorea
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\columnwidth]{figures/code-quality-noBB/code-quality-noBB.eps}
\caption{\label{fig:code-quality} Impact on running time of never-firing OSR points inserted inside hot code portions.}
\end{center}
\end{figure}
\fi

\ifauthorea{\newline}{}
\paragraph{OSR-point insertion overhead}
[...]
  

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  