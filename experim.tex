\section{Experimental Evaluation}
\label{se:experiments}

In this section we present a preliminar experimental study of our OSR technique in TinyVM, a proof-of-concept virtual machine based on LLVM's JIT compiler MCJIT. TinyVM supports interactive invocations of functions and it can compile LLVM IR either generated at run-time or loaded from disk. The main design goal behind TinyVM is the creation of an interactive environment for IR manipulation and JIT-compilation of functions: for instance, it allows the user to insert OSR points in loaded functions, run optimization passes on them or display their CFGs, repeatedly invoke a function for a specified amount of times and so on. TinyVM supports dynamic library loading and linking, and comes with a helper component for MCJIT that simplifies tasks such as handling multiple IR modules, symbol resolution in presence of multiple versions of a function, and tracking native code and other machine-level generated object such as Stackmaps.

TinyVM is thus an ideal playground to exercise our OSR technique, and we use it to run performance measurements on the shootout test suite, also known as the Computer Language Benchmark Game~\cite{shootout}. The list of benchmarks and their description is reported in Table~\ref{tab:shootout}.

\begin{table} 
    \begin{tabular}{ c c }
        Benchmark & Description \\ 
        b-trees & Adaptation of a GC bench for binary trees \\ 
        fannkuch & Fannkuch benchmark on permutations \\ 
        fasta & Generation of DNA sequences \\ 
        fasta-redux & Generation of DNA sequences (with lookup table) \\ 
        mbrot & Mandelbrot set generation \\ 
        n-body & N-body simulation of Jovian planets \\ 
        rev-comp & Reverse-complement of DNA sequences \\ 
        sp-norm & Eigenvalue calculation with power method \\ 
    \end{tabular} 
    \caption{\label{tab:shootout} Description of the shootout benchmarks} 
\end{table}

\subsection{Setup}

We generated the IR modules for our experiments with clang, starting from the C version of the shootout suite. No LLVM optimization passes were performed on the code other than {\em mem2reg}, which promotes memory references to be register references and is typically used in SSA form construction.

\subsection{Results}

\begin{itemize}
\item {\bf Message 1}: how much does a never-firing OSR point impact code quality? We run a program with one or more OSR points, and we measure the slowdown given by factors such as cache effects (due to code bloat), register pressure, etc. due to the presence of the OSR points.
\item {\bf Message 2}: what is the overhead of an OSR transition to the same function? We run a program with a controlled OSR transition, e.g., with a counter that fires the OSR. Here we measure the impact of the actual OSR call [we already tried this with the repeated addition microbenchmark simple\_loop\_SSA.ll].
\item {\bf Message 3}: what is the overhead of the library for inserting OSR points?
\end{itemize}

\paragraph{Impact on code quality}
In order to measure how much a never-firing OSR point might impact code quality, we analyzed the source-code structure of each benchmark and profiled its run-time behavior to identify performance-critical sections for OSR point insertion.

For iterative benchmarks, we insert an OSR point in the body of their hottest loops. We classify a loop as hottest when its body is executed for a very high cumulative number of iterations (e.g., from a few thousands up to billions) and it either calls the method with the highest {\em self} time in the program, or it performs the most computational-intensive operations for the program in its own body. These loops are natural candidates for OSR point insertion, as they can be used - as in the Jikes RVM - to enable more dynamic inlining opportunities, with the benefits from several control-flow (e.g., dead code elimination) and data-flow (e.g., constant propagation) optimizations based on the run-time values of the live variables. In the shootout benchmarks, the number of such loops is typically 1 (2 for {\tt spectral-norm}).

For recursive benchmarks, we insert an OSR point in the body of the method that accounts for the largest {\em self} execution time of the program. Such an OSR point might enable recompilation of the invoked method at a higher degree of optimization, [...]

In the shootout benchmarks, ({\tt binary-trees} and {\tt spectral-norm}) show a recursive pattern.
  
  
  
  
  
  
  
  
  
  
  
  
  