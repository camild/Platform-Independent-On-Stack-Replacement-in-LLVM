\section{Experimental Evaluation}
\label{se:experiments}

In this section we present an experimental study of our OSR technique in TinyVM, a proof-of-concept virtual machine based on LLVM's JIT compiler MCJIT. TinyVM supports interactive invocations of functions and it can compile LLVM IR either generated at run-time or loaded from disk. The main design goal behind TinyVM is the creation of an interactive environment for IR manipulation and JIT-compilation of functions: for instance, it allows the user to insert OSR points in loaded functions, run optimization passes on them or display their CFGs, repeatedly invoke a function for a specified amount of times and so on. TinyVM supports dynamic library loading and linking, and comes with a helper component for MCJIT that simplifies tasks such as handling multiple IR modules, symbol resolution in presence of multiple versions of a function, and tracking native code and other machine-level generated object such as Stackmaps.

TinyVM is thus an ideal playground to exercise our OSR technique, and we use it to run performance measurements on the shootout test suite, also known as the Computer Language Benchmark Game~\cite{shootout}. The list of benchmarks and their description is reported in Table [...].

\subsection{Setup}

We generated the IR modules for our experiments with clang, starting from the C version of the shootout suite. No LLVM optimization passes were performed on the code other than {\em mem2reg}, which promotes memory references to be register references and is typically used in SSA form construction.

\subsection{Results}

\begin{itemize}
\item {\bf Message 1}: how much does a never-firing OSR point impact code quality? We run a program with one or more OSR points, and we measure the slowdown given by factors such as cache effects (due to code bloat), register pressure, etc. due to the presence of the OSR points.
\item {\bf Message 2}: what is the overhead of an OSR transition to the same function? We run a program with a controlled OSR transition, e.g., with a counter that fires the OSR. Here we measure the impact of the actual OSR call [we already tried this with the repeated addition microbenchmark simple\_loop\_SSA.ll].
\item {\bf Message 3}: what is the overhead of the library for inserting OSR points?
\end{itemize}

\paragraph{Impact on code quality}
In order to measure how much a never-firing OSR point impacts code quality, we analyzed the code structure of each benchmark and profiled its run-time behavior with gprof to determine the most critical sections for OSR point insertion.
For recursive benchmarks ({\tt binary-trees} and {\tt spectral-norm}), we insert an OSR point in the body of the hottest method (i.e., the one accounting for the largest {\em self} execution time) of the program. For iterative benchmarks, we insert an OSR point on loops that are responsible for the largest fraction of cumulative invocations of the hottest method.
  
  
  
  
  
  
  
  